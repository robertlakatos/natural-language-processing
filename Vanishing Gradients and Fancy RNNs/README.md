# Word Window Classification, Neural Networks

## Materials

* [Lecture]()
* [Note]() 
* [Azure Guide]()
* [Practical Guide to VMs]()
* [Video](https://www.youtube.com/watch?v=QEw0qEa0E50&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&index=7)

## Assignments

## Suggested Readings

* [Sequence Modeling: Recurrent and Recursive Neural Nets (Sections 10.3, 10.5, 10.7-10.12)](http://www.deeplearningbook.org/contents/rnn.html)
* [Learning long-term dependencies with gradient descent is difficult (one of the original vanishing gradient papers)]()
* [On the difficulty of training Recurrent Neural Networks (proof of vanishing gradient problem)]()
* [Vanishing Gradients Jupyter Notebook (demo for feedforward networks)](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/lectures/vanishing_grad_example.html)
* [Understanding LSTM Networks (blog post overview)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

# [BACK TO THE TABLE OF CONTENTS](https://github.com/robertlakatos/natural-language-processing/blob/master/README.md)