# Natural Language Processing based on course of CS 224 of Stanford University

## Table of contents

1. [Introduction and Word Vectors](https://github.com/robertlakatos/natural-language-processing/blob/master/Introduction%20and%20Word%20Vectors/README.md)
2. Gensim word vectors example
3. Word Vectors 2 and Word Senses 
4. Python review session
5. Word Window Classification, Neural Networks, and PyTorch 
6. Matrix Calculus and Backpropagation 
7. Linguistic Structure: Dependency Parsing 
8. The probability of a sentence? Recurrent Neural Networks and Language Models
9. Vanishing Gradients and Fancy RNNs
10. Machine Translation, Seq2Seq and Attention
11. Practical Tips for Final Projects
12. Question Answering, the Default Final Project, and an introduction to Transformer architectures
13. ConvNets for NLP
14. Information from parts of words (Subword Models)
15. Contextual Word Representations: BERT
16.	Modeling contexts of use: Contextual Representations and Pretraining. ELMo and BERT
17. Natural Language Generation
18. Reference in Language and Coreference Resolution
19. Fairness and Inclusion in AI 
20. Constituency Parsing and Tree Recursive Neural Networks 
21. Virtual Office Hours with HuggingFace
22. Recent Advances in Low Resource Machine Translation 
23. Analysis and Interpretability of Neural NLP 

## References

* [Stanford CS 224N](http://web.stanford.edu/class/cs224n/)
* [keras](https://keras.io/)
* [pytorch](https://pytorch.org/)
* [tensorflow](https://www.tensorflow.org/)
* [python](https://www.python.org/)